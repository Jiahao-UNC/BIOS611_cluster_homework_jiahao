---
title: "BIOS611 Clustering Homework"
format: html
execute:
  echo: true
  warning: false
---

# Task 1 — Hypercube Clusters, K-means, and the Gap Statistic

## Objective
Evaluate how accurately the Gap Statistic (with Tibshirani’s selection rule) recovers the true number of clusters in synthetic **hypercube** datasets as the **inter-cluster separation** (edge length *L*) decreases and the **dimension** *n* varies.

## Data Generation
For each dimension \( n \in \{6,5,4,3,2\} \), we construct a hypercube in \(\mathbb{R}^n\) whose vertices are at coordinates \(\{0, L\}^n\). Each vertex is treated as a true cluster center. The true number of clusters is therefore \(K_{\text{true}} = 2^n\). Around each center, we sample points from an **isotropic Gaussian**:
\[
X \sim \mathcal{N}(\mu, \sigma^2 I_n), \quad \sigma = 1.0,
\]
with **100 points per cluster**. For each \(n\), we sweep the edge length \(L\) from 10 down to 1 (step 1), which gradually reduces inter-cluster separation.

## Clustering + Model Selection
For each dataset \((n, L)\), we run **K-means** with `nstart = 20` and `iter.max = 50`. To estimate the number of clusters \( \hat{k} \), we apply the **Gap Statistic** (R `cluster::clusGap`) with:
- Reference bootstrap samples: **B = 50** (balanced for stability and runtime).
- Upper cap for search: \(K_{\max} = \min(2^n + 3, 12)\).

**Selection rule (Tibshirani et al.)**: choose the smallest \(k\) such that
\[
\text{Gap}(k) \ge \text{Gap}(k+1) - \text{SE}(k+1).
\]

## Outputs
- **Summary table**: `results/tables/t1_gap_summary.csv` (columns: `n`, `side_length`, `K_true`, `k_hat`, etc.).
- **Failure-point table**: `results/tables/t1_failure_points.csv`, which records for each dimension \(n\) the first edge length \(L_{\text{fail}}\) (scanning from large to small) where \( \hat{k} < K_{\text{true}} \).
- **Figure**: `results/figs/t1_gap_curve.png` showing \(\hat{k}\) versus \(L\), faceted by \(n\), with dashed horizontal lines at \(K_{\text{true}}\) and a red dotted vertical line at \(L_{\text{fail}}\).

## Results (Quick View)

![](../results/figs/t1_gap_curve.png){width=92%}

```{r}
# Failure points per dimension
fail <- read.csv("../results/tables/t1_failure_points.csv")
fail

# Task 2 — Concentric Shells, Spectral Clustering, and the Gap Statistic

## Idea in one breath
We now switch to **concentric rings** in 2D. Each ring is a true cluster (so the true cluster count is the number of shells). We gradually shrink the overall scale via the **max radius** \(R\), from 10 down to 1, and run **spectral clustering** (epsilon-neighborhood graph, \(d_{\text{th}}=1.0\)) inside the Gap Statistic to estimate \(\hat{k}\).

## What to look for
- Dashed horizontal line = true cluster count (number of shells).  
- Red dotted vertical line = the first radius \(R_{\text{fail}}\) where we begin to **underestimate** \(\hat{k}<K_{\text{true}}\).  
- Red points = underestimates.

![](../results/figs/t2_gap_curve.png){width=92%}

```{r}
fail2 <- read.csv("../results/tables/t2_failure_points.csv")
fail2 EOF

# Task 2 — Failure Point Explained & Effect of d_threshold

## What the failure point means as max_radius decreases
We place \(n_{\text{shells}}\) concentric rings in 2D; each ring is a true cluster. Their radii are
\(r_s = s \cdot \frac{\text{max\_radius}}{n_{\text{shells}}}\) for \(s=1,\dots,n_{\text{shells}}\), so the radial gap between adjacent rings is
\(\Delta r = \frac{\text{max\_radius}}{n_{\text{shells}}}\). Spectral clustering builds an \(\varepsilon\)-neighborhood graph (connect two points if their distance < \(d_{\text{th}}\)). With small noise, same-ring points connect densely while different rings remain disconnected—**if** the gap is large.

As we **decrease max_radius**, the gap \(\Delta r\) shrinks. Sooner or later, due to radial jitter and angular differences, some cross-ring pairs fall **within** \(d_{\text{th}}\). The graph starts to form **bridges** across rings; the Laplacian’s smallest eigenvectors then mix rings, and K-means on those eigenvectors merges clusters. That’s when the Gap+spectral pipeline begins to **underestimate** the true number of rings. The **failure point \(R_{\text{fail}}\)** in the plot is exactly the first max_radius (scanned from large to small) where the estimate \(\hat{k}\) drops below the truth.

A rough mental model:
\[
\text{failure when}\quad \Delta r \approx \frac{\text{max\_radius}}{n_{\text{shells}}} \lesssim \text{(effective proximity at scale } d_{\text{th}} \text{ + noise)}.
\]

## How changing \(d_{\text{th}}\) shifts the failure point
- **Smaller \(d_{\text{th}}=0.8\)** (sparser graph): harder to create accidental cross-ring edges; underestimation appears **later**.  
  ⇒ \(R_{\text{fail}}\) moves **down** (you need a **smaller** max_radius before failure).
- **Larger \(d_{\text{th}}=1.2\)** (denser graph): easier to bridge adjacent rings; underestimation appears **earlier**.  
  ⇒ \(R_{\text{fail}}\) moves **up** (failure at a **larger** max_radius).

Rule of thumb:
\[
R_{\text{fail}} \;\propto\; n_{\text{shells}} \cdot d_{\text{th}} \times \text{(noise factor)}.
\]
So \(R_{\text{fail}}\) grows with \(d_{\text{th}}\) and with stricter separation needs in higher noise or denser sampling.

## Practical notes
- More noise or more points per ring → more chances of cross-ring neighbors → earlier failure (larger \(R_{\text{fail}}\)).  
- If rings are angularly clumped, local bridges form sooner.  
- Tuning \(d_{\text{th}}\) is key: too small fragments the graph; too large over-connects it.


# Task 2 — Sensitivity to `d_threshold` (0.8 vs 1.0 vs 1.2)

We compare three epsilon thresholds for the spectral graph: **0.8**, **1.0**, and **1.2**.  
Smaller thresholds make the graph sparser (protect against over-connecting rings),  
while larger thresholds make it denser (rings merge sooner).

**Expected pattern:** \(R_{\text{fail}}(0.8) \lt R_{\text{fail}}(1.0) \lt R_{\text{fail}}(1.2)\).

### Curves
*(Top: tighter graph; Middle: baseline; Bottom: looser graph)*

![](../results/figs/t2_gap_curve_08.png){width=92%}
![](../results/figs/t2_gap_curve.png){width=92%}
![](../results/figs/t2_gap_curve_12.png){width=92%}

### Failure radii table
```{r}
fail08 <- read.csv("../results/tables/t2_failure_points_08.csv")
fail10 <- read.csv("../results/tables/t2_failure_points.csv")
fail12 <- read.csv("../results/tables/t2_failure_points_12.csv")

data.frame(
  d_threshold = c(0.8, 1.0, 1.2),
  R_fail      = c(fail08$R_fail[1], fail10$R_fail[1], fail12$R_fail[1])
)

## Task 2 — Sensitivity to the distance threshold (0.8 vs 1.0 vs 1.2)

We now compare three epsilon thresholds used to build the similarity graph.  
A smaller threshold makes the graph sparser and delays the failure point;  
a larger threshold makes it denser and causes earlier underestimation.

![](../results/figs/t2_gap_curve_m4_elbow_d08.png){width=90%}
![](../results/figs/t2_gap_curve_m4_tib_d10.png){width=90%}
![](../results/figs/t2_gap_curve_m4_elbow_d12.png){width=90%}

```{r}
fail08 <- read.csv("../results/tables/t2_failure_points_m4_elbow_d08.csv")
fail10 <- read.csv("../results/tables/t2_failure_points_m4_tib_d10.csv")
fail12 <- read.csv("../results/tables/t2_failure_points_m4_elbow_d12.csv")
data.frame(
  d_threshold = c(0.8, 1.0, 1.2),
  R_fail = c(fail08$R_fail[1], fail10$R_fail[1], fail12$R_fail[1])
)


## Task 2 — Note on 3D shells and why \(\hat{k}\) can be 1 across radii

In our 3D concentric-shell setup we sweep the maximum radius while keeping a **fixed distance threshold** \(d_{\text{th}}\) and a **fixed number of points per shell**. Because a sphere’s surface area scales as \(4\pi r^2\), increasing the radius **reduces point density** on each shell and **increases** the typical within-shell neighbor spacing. With a single global \(d_{\text{th}}\), the \(\varepsilon\)-graph may thus be **too sparse within shells** at large radii, while at very small radii it may be **over-connected across shells**. Either way, the spectral embedding can degenerate and the **Gap statistic may consistently select \(\hat{k}=1\)**, which matches the behavior we observed in our runs.

We **leave the code unchanged** and record this as a limitation of using one global scale across widely different radii.
