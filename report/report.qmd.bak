---
title: "BIOS611 Clustering Homework"
format: html
execute:
  echo: true
  warning: false
---

# Task 1 — Hypercube Clusters, K-means, and the Gap Statistic

## Objective
Evaluate how accurately the Gap Statistic (with Tibshirani’s selection rule) recovers the true number of clusters in synthetic **hypercube** datasets as the **inter-cluster separation** (edge length *L*) decreases and the **dimension** *n* varies.

## Data Generation
For each dimension \( n \in \{6,5,4,3,2\} \), we construct a hypercube in \(\mathbb{R}^n\) whose vertices are at coordinates \(\{0, L\}^n\). Each vertex is treated as a true cluster center. The true number of clusters is therefore \(K_{\text{true}} = 2^n\). Around each center, we sample points from an **isotropic Gaussian**:
\[
X \sim \mathcal{N}(\mu, \sigma^2 I_n), \quad \sigma = 1.0,
\]
with **100 points per cluster**. For each \(n\), we sweep the edge length \(L\) from 10 down to 1 (step 1), which gradually reduces inter-cluster separation.

## Clustering + Model Selection
For each dataset \((n, L)\), we run **K-means** with `nstart = 20` and `iter.max = 50`. To estimate the number of clusters \( \hat{k} \), we apply the **Gap Statistic** (R `cluster::clusGap`) with:
- Reference bootstrap samples: **B = 50** (balanced for stability and runtime).
- Upper cap for search: \(K_{\max} = \min(2^n + 3, 12)\).

**Selection rule (Tibshirani et al.)**: choose the smallest \(k\) such that
\[
\text{Gap}(k) \ge \text{Gap}(k+1) - \text{SE}(k+1).
\]

## Outputs
- **Summary table**: `results/tables/t1_gap_summary.csv` (columns: `n`, `side_length`, `K_true`, `k_hat`, etc.).
- **Failure-point table**: `results/tables/t1_failure_points.csv`, which records for each dimension \(n\) the first edge length \(L_{\text{fail}}\) (scanning from large to small) where \( \hat{k} < K_{\text{true}} \).
- **Figure**: `results/figs/t1_gap_curve.png` showing \(\hat{k}\) versus \(L\), faceted by \(n\), with dashed horizontal lines at \(K_{\text{true}}\) and a red dotted vertical line at \(L_{\text{fail}}\).

## Results (Quick View)

![](../results/figs/t1_gap_curve.png){width=90%}

```{r}
# Preview top rows of the summary and failure points
summary <- read.csv("../results/tables/t1_gap_summary.csv")
fail    <- read.csv("../results/tables/t1_failure_points.csv")
head(summary, 10)
fail

# Task 1 — Results (Plain-English Recap)

## What we looked at
We generated hypercube clusters in different dimensions (n = 2, 3, 4, 5, 6).  
For each dimension, every vertex of the n-dimensional cube was a true cluster center, so the true number of clusters is \(2^n\). We kept noise at \(\sigma = 1\) and moved the cluster centers farther/closer by changing the edge length \(L\) from 10 down to 1.

## How we measured success
For each \((n, L)\), we ran K-means (`nstart=20`, `iter.max=50`) and used the Gap Statistic to pick \(\hat{k}\). The rule is the standard Tibshirani one: choose the smallest \(k\) with \(\text{Gap}(k) \ge \text{Gap}(k+1) - \text{SE}(k+1)\).

## What the figure says at a glance
- The dashed **horizontal** lines are the true cluster counts \(2^n\).  
- The **red dotted vertical** line in each facet is the first edge length where we **start underestimating** the number of clusters — we call it \(L_{\text{fail}}\).  
- Black points are “okay” estimates; **red points** are underestimates.

![](../results/figs/t1_gap_curve.png){width=92%}

## Where the method starts to slip
The table below prints, for each dimension \(n\), the first edge length where \(\hat{k} < 2^n\) when we scan \(L\) from big to small. That’s our practical “failure point.”

```{r}
fail <- read.csv("../results/tables/t1_failure_points.csv")
fail ```
